# This file provides the realization of the algorithm for multilevel gradient estimators.
import numpy as np
obj=2
d=20


S0=0.5
r=0.1

def generateS(n,M,B,mu):
    S=S0*np.ones(d)
    delta=1/(M**n)
    wt=np.random.normal(0, delta**(0.5), (M**n,d))
    
    for i in range(M**n):
        for j in range(d):
            S[j]=S[j]+ mu[j]*S[j]*delta + S[j]*np.dot(B[j],wt[i])
        
    return S,(M**n)*d

def multilevel(n,M,B,mu):
    S_lowl=S0*np.ones(d)
    S_highl=S0*np.ones(d)
    delta=1/(M**n)
    
    wt=np.random.normal(0, delta**(0.5), (M**n,d))
    wt_lowl=np.zeros(d)
    
    for i in range(M**n):
        wt_lowl=wt_lowl+wt[i]
        for j in range(d):
            S_highl[j]=S_highl[j]+ mu[j]*S_highl[j]*delta + S_highl[j]*np.dot(B[j],wt[i])
           
        if i % M == M-1 :
            for j in range(d):
                S_lowl[j]=S_lowl[j] + mu[j]*S_lowl[j]*delta*M + S_lowl[j]*np.dot(B[j],wt_lowl)
            wt_lowl=np.zeros(d)
    return S_highl, S_lowl, (M**n+M**(n-1))*d



def multilevelgradient(M, m ,N_0,m_0, x, beta, rho, r, t, B,mu):
    
    
    complexity=0
    S1= S0*np.exp(r)
    
    approxcumgrad=np.zeros(d)
    h = (d**(-3/2))*(t**(-rho))
    for k in range(m+1):
            N= int(np.ceil(N_0*(d**4)*(t**(r+2*rho))*(m+1)*(M**(-(k+1)))))
            
            if k ==0 :
                for j in range(N):
                    Z=np.random.normal(0,1,d)
                    S,runcomplex=generateS(0,M,B,mu)
                    Sreg=S-S1*np.ones(d)
                    currentfunctionvalue= (np.dot(Sreg,x)+S1-obj)**2
                    onestepfunctionvalue= (np.dot(Sreg,x+h*Z)+S1-obj)**2
                    currentapproxgrad= Z*(onestepfunctionvalue-currentfunctionvalue)/h
                    
                    approxcumgrad=approxcumgrad + currentapproxgrad/N
                    complexity=complexity + runcomplex
            else:
                for j in range(N):
                    Z=np.random.normal(0,1,d)
                    S,S_low, runcomplex2=multilevel(k,M,B,mu)
                    Sreg=S-S1*np.ones(d)
                    Sreg_low=S_low-S1*np.ones(d)
                    currentfunctionvalue= (np.dot(Sreg,x)+S1-obj)**2
                    onestepfunctionvalue= (np.dot(Sreg,x+h*Z)+S1-obj)**2
                    gradient_high= Z*(onestepfunctionvalue-currentfunctionvalue)/h
                    currentfunctionvalue_low= (np.dot(Sreg_low,x)+S1-obj)**2
                    onestepfunctionvalue_low= (np.dot(Sreg_low,x+h*Z)+S1-obj)**2
                    gradient_low=Z*(onestepfunctionvalue_low-currentfunctionvalue_low)/h
                    approxcumgrad=approxcumgrad+ (gradient_high-gradient_low)/N
                    complexity=complexity + runcomplex2
    return approxcumgrad, complexity
    
def multilevelsgd(M, gamma_0,N_0,m_0, x_0, beta, rho, r, t, B,mu):
    x=x_0
    complexity=0
    complexity_list=np.zeros(t)
    loss=np.zeros(t)
    
    for i in range(1,t+1):
        m= int(np.ceil(4*np.log(m_0*d*(i**rho))/np.log(M)))
        approxgrad, addcomplex= multilevelgradient(M, m, N_0,m_0, x, beta, rho, r, i, B,mu)
        
        x = x - gamma_0* approxgrad / (i**beta)
        for j in range(d):
            if x[j] < 0:
                x[j]=0
        if np.sum(x)>1:
            x = x/np.sum(x)
        complexity = complexity + addcomplex
        complexity_list[i-1]=complexity
        optimal=np.array([0.01626743, 0.04011657, 0.07892135, 0.00560909 ,0.07032327, 0.05372073,
 0.04716372, 0.00973844, 0.01425402, 0.02534226, 0.03015609, 0.00872706,
 0.03512897, 0.03519695, 0.03683114, 0.02742573, 0.01621328, 0.05961481,
 0.0232715,  0.03422605])
        loss[i-1]=np.dot(x-optimal,x-optimal)
        
    
    return x, complexity_list,loss


complexity=np.zeros(200)
B=np.array([[1.      ,   0.15752525, 0.16653036, 0.11867184, 0.09677731, 0.07523558,
  0.13219056, 0.14916727, 0.19439135, 0.05457639, 0.10987653, 0.09950581,
  0.15196475, 0.17587578, 0.17826184, 0.14618088, 0.10107566, 0.16292886,
  0.05364244, 0.05353046],
 [0.11201547, 1.        , 0.13112886, 0.18180827, 0.0888813,  0.14313753,
  0.10649524, 0.06009596, 0.08476188, 0.09081348, 0.07083317, 0.05835514,
  0.08263706, 0.12877411, 0.08211146, 0.11432411, 0.07000079, 0.18986623,
  0.1208125,  0.10550681],
 [0.07643004, 0.1155529,  1.        , 0.08088282, 0.07077095, 0.09339357,
  0.07486455, 0.11362563, 0.17330433, 0.08630449, 0.10523481, 0.12207231,
  0.16062058, 0.16878496, 0.06238724, 0.13712756, 0.1418305,  0.07128715,
  0.18130226, 0.05388989],
 [0.14896896, 0.06067856, 0.18264222, 1. ,        0.1387804,  0.18391876,
  0.16767403, 0.14488525, 0.06476551, 0.10087458, 0.15049564, 0.18625632,
  0.08263484, 0.17564358, 0.08281962, 0.09645005, 0.05593739, 0.10689313,
  0.1143305,  0.17908605],
 [0.0864878,  0.14358267, 0.09605479, 0.16077532, 1.    ,     0.06181471,
  0.08490168, 0.1374667,  0.05658673, 0.13972415, 0.13964343, 0.12350389,
  0.09327097, 0.13728118, 0.1063533,  0.05139945, 0.05737741, 0.15796923,
  0.07974313, 0.17834964],
 [0.150117,   0.11062953, 0.05984528, 0.14330751, 0.09655558, 1.,
  0.10622685, 0.06549052, 0.09209558, 0.11095411, 0.08094944, 0.18737155,
  0.08812487, 0.05457219, 0.11027203, 0.08704935, 0.06701945, 0.19758349,
  0.07953508, 0.0629807 ],
 [0.05146487, 0.16068095, 0.07458659, 0.07483225, 0.19430621, 0.19515934,
  1.     ,    0.16858752, 0.19387922, 0.09392485, 0.05704708, 0.18552986,
  0.08611423, 0.06263876, 0.10464521, 0.18123238, 0.13007801, 0.1403558,
  0.05689608, 0.1664432 ],
 [0.10264959, 0.177262 ,  0.05628569, 0.08808193, 0.05864721, 0.06849913,
  0.11787823, 1.       ,  0.17331237, 0.10191878, 0.1456592 , 0.12726538,
  0.14951778, 0.17074052, 0.1834097,  0.09748055, 0.15039612, 0.09413904,
  0.14520211, 0.19651847],
 [0.09983979, 0.10966413, 0.14344982, 0.19314896, 0.17660647, 0.07771667,
  0.09980068, 0.11740586, 1.       ,  0.08720403, 0.0844232,  0.09760949,
  0.13626905, 0.13096873, 0.13301598, 0.18482372, 0.06734066, 0.17718029,
  0.1614274,  0.15678586],
 [0.14198046, 0.08263715, 0.13189791, 0.15253917, 0.06748302, 0.19653523,
  0.17307984, 0.05211751, 0.14195253, 1.      ,   0.18744718, 0.15313579,
  0.11286365, 0.0734899,  0.15498908, 0.05801772, 0.11488523, 0.06066383,
  0.06718873, 0.15644626],
 [0.1934357 , 0.14367349, 0.12521643, 0.07507107, 0.10149373, 0.07706237,
  0.08573649, 0.17330399, 0.17377124, 0.11147014, 1.      ,   0.13133406,
  0.06981849, 0.08977075, 0.08470545, 0.16038175, 0.08560722, 0.1188503,
  0.0843953,  0.09097091],
 [0.15637338, 0.08671653, 0.17044023, 0.12717139, 0.07010219, 0.10928007,
  0.14121187, 0.14827389, 0.10365732, 0.06528278, 0.1764474 , 1.,
  0.14850885, 0.16807625, 0.13166601, 0.15813163, 0.13487492, 0.17974976,
  0.18488185, 0.05428861],
 [0.05968522, 0.08895139, 0.18406823, 0.0563623,  0.19091576, 0.07428385,
  0.12689825, 0.18436015, 0.18107658, 0.19291548, 0.15898577, 0.13358598,
  1.     ,    0.10135485, 0.08523448, 0.19712641, 0.11181719, 0.09450093,
  0.14432667, 0.11818481],
 [0.10692301, 0.16685689, 0.12417314, 0.17072216, 0.16048097, 0.15083653,
  0.05539885, 0.15507638, 0.0625216 , 0.08424477, 0.09234757, 0.06314329,
  0.09229295, 1.      ,   0.10612203, 0.16871766, 0.18777604, 0.09917106,
  0.13993322, 0.1028148 ],
 [0.19690033, 0.15853205, 0.08198193, 0.12724222, 0.08515097, 0.19030319,
  0.16794961, 0.11879913, 0.1729463,  0.11643521, 0.08987841, 0.08659558,
  0.16693699, 0.08403047, 1.      ,   0.17000088, 0.05813795, 0.05077853,
  0.15378446, 0.10538308],
 [0.12707928, 0.09781607, 0.16996605, 0.12786329, 0.11676526, 0.16579219,
  0.17630007, 0.16153483, 0.12677119, 0.10391732, 0.14969296, 0.05558296,
  0.15390289, 0.06907763, 0.12732501, 1.     ,    0.17430075, 0.08555243,
  0.06795367, 0.12684039],
 [0.19593349, 0.12350508, 0.05607213, 0.17085179, 0.0520376,  0.08977189,
  0.12952993, 0.10496384, 0.15378344, 0.1022494,  0.19259335, 0.10896288,
  0.08527692, 0.0604161,  0.14588518, 0.16722247, 1.        , 0.15204481,
  0.16006551, 0.17107288],
 [0.05609709, 0.13424226, 0.05244318, 0.19881046, 0.15964195, 0.11667001,
  0.056602 ,  0.14760099, 0.17602806, 0.15414026, 0.12661458, 0.13400081,
  0.08591097, 0.08599747, 0.10458315, 0.1109717,  0.08934129, 1.,
  0.06399498, 0.18121954],
 [0.07687271, 0.08323533, 0.12237926, 0.094905,   0.08604249, 0.18692375,
  0.14199201, 0.15827633, 0.12285279, 0.19802024, 0.14615647, 0.09877614,
  0.14480651, 0.08780457, 0.13017566, 0.14966874, 0.06211811, 0.10785882,
  1.     ,    0.17592812],
 [0.16783162, 0.07350135, 0.11951176, 0.12875636, 0.18991271, 0.10932718,
  0.10487624, 0.07023484, 0.16794027, 0.06800127, 0.18953285, 0.08669304,
  0.16404454, 0.1772242 , 0.17004407, 0.18903556, 0.10741395, 0.08620808,
  0.11906912, 1.        ]])
mu=np.array([0.63207021, 0.61992663, 0.73172743, 0.64518767, 0.73786222, 0.62169804,
 0.74762364, 0.62465105, 0.69609411, 0.62514971, 0.61647187, 0.65487383,
 0.75162138, 0.66063224, 0.68806338, 0.70626801, 0.61251768, 0.7479484,
 0.6509934,  0.75783933])
complexity=np.zeros(200)
loss=np.zeros(200)


for k in range(50):
    xfinal, complexity_final,add_loss=multilevelsgd(2, 10,10,2, np.zeros(d), 1, 1/2, 0, 200, B,mu)
    loss=loss+ add_loss
    print(k)
    print(loss)

print(loss/50)
print(complexity_final)